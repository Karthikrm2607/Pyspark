What is PySpark?


PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. If youâ€™re already familiar with Python and libraries such as Pandas, then PySpark is a good language to learn to create more scalable analyses and pipelines.

Apache Spark is basically a computational engine that works with huge sets of data by processing them in parallel and batch systems. Spark is written in Scala, and PySpark was released to support the collaboration of Spark and Python. In addition to providing an API for Spark, PySpark helps you interface with Resilient Distributed Datasets (RDDs) by leveraging the Py4j library.


Above are the documents attached, 
(to execute them , need to upload all files to databricks via this link
link : https://community.cloud.databricks.com/
Goto Data>>CreateTable>>UploadAllFiles
GoTo Compute/Create>>NewCluster>>run)
The code to be followed is attached as (Spark RDD's.HTML) html file
